Bu derste 
1. mysql veri tabanında bir veri tabanı ve tablo yaratma 
	1.1. mysql veri tabanına shell ile bağlanma:
	[root@sandbox-hdp ~]# mysql -u root -p
		Şifre: hadoop
		
	1.2. yeni bir veri tabanı yaratma:
	[root@sandbox-hdp ~]# create database azhadoop;
	
	1.3. Mevcut veri tabanlarını görüntüleme:
	mysql> show databases;
		+--------------------+
		| Database           |
		+--------------------+
		| information_schema |
		| azhadoop           |
		| hive               |
		| mysql              |
		| performance_schema |
		| ranger             |
		+--------------------+
		6 rows in set (0.04 sec)
		
	1.4. azhadoop vertabanına yetki
mysql> GRANT ALL PRIVILEGES ON azhadoop.* TO 'root'@'localhost';
mysql> GRANT ALL PRIVILEGES ON azhadoop.* TO 'root'@'sandbox-hdp.hortonworks.com';

	1.5. Kullanılacak veri tabanını seçme:
	mysql> use azhadoop;
	Database changed
	
	1.6. Veri tabanındaki tabloları listeleme:
	mysql> show tables;
	Empty set (0.00 sec)
	
	boş
	
	
2. iris.csv doyasının içeriği bu tabloya aktarma

	2.1. iris veri setine uygun tablo yaratma 
	
	mysql> create table iris_mysql(SepalLengthCm double, SepalWidthCm double, PetalLengthCm double, PetalWidthCm double, Species VARCHAR(20));

	2.1. iris.csv dosyasını /root dizinine taşıma ve başlık satırını silme
		[root@sandbox-hdp ~]# wget https://raw.githubusercontent.com/erkansirin78/hadoop-spark-for-datascientists/master/Datasets/iris.csv
		
	2.2. 
	[root@sandbox-hdp ~]# vi iris.csv
		ile dosyanın başlık satırını sil 
		mysql'e tekrar bağlan  

		mysql> use azhadoop;

		mysql>
		LOAD DATA LOCAL INFILE '/root/iris.csv' INTO TABLE iris_mysql
		FIELDS TERMINATED BY ',' 
		ENCLOSED BY '"' 
		LINES TERMINATED BY '\n'
		(SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species);




mysql çıkış.
 \q:


3. sqoop eval komutu

[root@sandbox-hdp ~]# sqoop eval --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--username root --password hadoop --query "select * from iris_mysql limit 5"

Bu komutun sonunda ekrana sorgu sonucu gelmelidir.

5. Ambari'den Atlas servisini siliyorum. Çünkü denemelerimde sürekli sqoop'un çalışmasını engellediğini gördüm.

6. sqoop import ile mysql'den hdfs'e veri aktarma 
import edilecek dizini hdfs'te yaratalım:
[maria_dev@sandbox-hdp ~]$ hdfs dfs /user/maria_dev/sqoop_import

Sqoop import'u çalştıralım:

[maria_dev@sandbox-hdp ~]$ sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver --username root --password hadoop \
--table iris_mysql --m 1 --target-dir /user/maria_dev/sqoop_import/iris

Hedef dizinde kontrol yapalım:
Ambari FilesView ile /user/maria_dev/sqoop_import/iris
diznini kontrol edelim ve iris verisetini görelim.
part-m-00000

Yukarıda --m 1 parametresini mapping sayısı çin kullanıyoruz. Şayet tablomuzda artan sıralı bir anahtar olsaydı
bunu daha fazla parça ile yapabilirdik. Ancak olmadığı için mecbur --m 1 yapıyoruz.



7. sqoop import ile mysql'den Hive'a veri aktarma 

deneme-1
sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop --username root --password hadoop --table iris_mysql --target-dir /tmp/hive_temp --fields-terminated-by "," --hive-import --create-hive-table --hive-table azhadoop.iris_hive

19/04/27 13:35:59 ERROR manager.SqlManager: Error reading from database: java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@4b8ee4de is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries.
java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@4b8ee4de is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries.


deneme-2

sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--username root --password hadoop \
--query "select * from iris_mysql WHERE \$CONDITIONS" \
--m 1 --hive-import --hive-table azhadoop.iris_hive --target-dir /tmp/hive_temp



sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop --username root --password hadoop --query "select * from iris_mysql WHERE \$CONDITIONS" --m 1 --hive-import --hive-table azhadoop.iris_hive --target-dir /tmp/hive_temp
